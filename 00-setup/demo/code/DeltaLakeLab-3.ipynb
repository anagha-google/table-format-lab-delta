{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.9"}, "serverless_spark": "{\"name\":\"projects/delta-labv7/locations/us-central1/sessions/delta-lake-lab-17036\",\"uuid\":\"7d155061-0492-4917-9cc5-30c0a975ff3e\",\"createTime\":\"2023-11-01T20:13:53.591330Z\",\"jupyterSession\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://4ezvzeq5xrakrgsx2ygb5k3zii-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/?eventLogDirFilter=7d155061-0492-4917-9cc5-30c0a975ff3e\"}},\"state\":\"ACTIVE\",\"stateTime\":\"2023-11-01T20:15:11.987039Z\",\"creator\":\"admin@jayoleary.altostrat.com\",\"runtimeConfig\":{\"version\":\"2.0.45\",\"properties\":{\"spark:spark.jars.packages\":\"io.delta:delta-core_2.13:2.1.0\",\"spark:spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\"spark:spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.dynamicAllocation.executorAllocationRatio\":\"0.3\",\"spark:spark.eventLog.dir\":\"gs://dll-sphs-bucket-697607461278/7d155061-0492-4917-9cc5-30c0a975ff3e/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"dll-lab-sa@delta-labv7.iam.gserviceaccount.com\",\"subnetworkUri\":\"spark-snet\",\"idleTtl\":\"3600s\",\"ttl\":\"86400s\"},\"peripheralsConfig\":{\"metastoreService\":\"projects/delta-labv7/locations/us-central1/services/dll-hms-697607461278\",\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/delta-labv7/regions/us-central1/clusters/dll-sphs-697607461278\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2023-11-01T20:13:53.591330Z\"}]}", "serverless_spark_kernel_name": "remote-0ee56a838219c50032816e30-pyspark", "vscode": {"interpreter": {"hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}, "kernelspec": {"name": "remote-0ee56a838219c50032816e30-pyspark", "display_name": "\ud83d\udd12[RESERVED] PySpark on delta-lake-lab-17036: Dataproc session in us-central1 (Remote)", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Delta Lake Lab \n## Unit 3: Delta Table Utilities\n\nThis lab is powered by Dataproc Serverless Spark.\n\nIn the previous unit-\n1. We created a base table in parquet\n\nIn this unit, we will -\n1. Create a base delta table off of the parquet base table loan_db.loans_by_state_parquet\n2. Take a peek under the hood of the Delta table\n3. Review the delta transaction log\n4. Look at delta table details\n5. Look at delta table history\n6. Create a manifest file for an unpartitioned delta table\n7. Create a Biglake table in BigQuery for the unpartitioned table\n8. Compare Biglake output to delta table output\n9. Create a manifest file for an partitioned delta table\n10. Create a Biglake table in BigQuery for the partitioned table\n11. Compare Biglake output to delta table output\n12. Review entries in the Hive Metastore (Dataproc Metastore Service)", "metadata": {}, "id": "4529a790-8425-4600-841a-4b094b82eaa8"}, {"cell_type": "markdown", "source": "### 1. Imports", "metadata": {}, "id": "1a31c4fd-d465-4f52-8e56-3775bf499abc"}, {"cell_type": "code", "source": "import pandas as pd\n\nfrom pyspark.sql.functions import month, date_format\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\n\nfrom delta.tables import *\n\nfrom google.cloud.exceptions import BadRequest\nfrom google.cloud import bigquery\n\nimport sqlparse\nimport warnings\nwarnings.filterwarnings('ignore')", "metadata": {"trusted": true}, "execution_count": 5, "outputs": [], "id": "1321bce9-178c-4065-8187-0a5728c1a370"}, {"cell_type": "markdown", "source": "### 2. Create a Spark session powered by Cloud Dataproc ", "metadata": {}, "id": "ed295a74-ed1d-4b5d-831a-1b5dcf73c36f"}, {"cell_type": "code", "source": "spark = SparkSession.builder.appName('Loan Analysis').getOrCreate()\nspark\n\n", "metadata": {"trusted": true}, "execution_count": 6, "outputs": [{"name": "stderr", "text": "23/11/01 20:25:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n", "output_type": "stream"}, {"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "<pyspark.sql.session.SparkSession at 0x7f435d83cbb0>", "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gdpic-srvls-session-7d155061-0492-4917-9cc5-30c0a975ff3e-m.us-central1-c.c.delta-labv7.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://gdpic-srvls-session-7d155061-0492-4917-9cc5-30c0a975ff3e-m:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "}, "metadata": {}}], "id": "b383d5ab-a0b9-45ab-a232-34d88f2a0065"}, {"cell_type": "markdown", "source": "### 3. Declare variables", "metadata": {}, "id": "63cd13e6-f3f5-4f2c-b4fc-d7e2660c6206"}, {"cell_type": "code", "source": "project_id_output = !gcloud config list --format \"value(core.project)\" 2>/dev/null\nPROJECT_ID = project_id_output[0]\nprint(\"PROJECT_ID: \", PROJECT_ID)", "metadata": {"trusted": true}, "execution_count": 7, "outputs": [{"name": "stdout", "text": "PROJECT_ID:  delta-labv7\n", "output_type": "stream"}], "id": "5596e31b-749a-4702-8879-6f05f9ff0c2d"}, {"cell_type": "code", "source": "project_name_output = !gcloud projects describe $PROJECT_ID | grep name | cut -d':' -f2 | xargs\nPROJECT_NAME = project_name_output[0]\nprint(\"PROJECT_NAME: \", PROJECT_NAME)", "metadata": {"trusted": true}, "execution_count": 8, "outputs": [{"name": "stdout", "text": "PROJECT_NAME:  delta-labv7\n", "output_type": "stream"}], "id": "471c6743-a058-462b-851a-f34323f36243"}, {"cell_type": "code", "source": "project_number_output = !gcloud projects describe $PROJECT_ID | grep projectNumber | cut -d':' -f2 | xargs\nPROJECT_NUMBER = project_number_output[0]\nprint(\"PROJECT_NUMBER: \", PROJECT_NUMBER)", "metadata": {"trusted": true}, "execution_count": 9, "outputs": [{"name": "stdout", "text": "PROJECT_NUMBER:  697607461278\n", "output_type": "stream"}], "id": "61929dc6-a083-433c-8a13-3d39d9c4a4a1"}, {"cell_type": "code", "source": "DATA_LAKE_ROOT_PATH= f\"gs://dll-data-bucket-{PROJECT_NUMBER}\"\nDELTA_LAKE_DIR_ROOT = f\"{DATA_LAKE_ROOT_PATH}/delta-consumable\"", "metadata": {"trusted": true}, "execution_count": 10, "outputs": [], "id": "f8b79fd2-5243-41a4-ae87-e7f9dd87cf20"}, {"cell_type": "markdown", "source": "### 4. Peek under the hood of our Delta Lake table (loan_db.loans_by_state_delta)", "metadata": {}, "id": "ed74d358-e90b-46d0-b81c-1e29c25ccac5"}, {"cell_type": "code", "source": "!gsutil ls -r $DELTA_LAKE_DIR_ROOT", "metadata": {"trusted": true}, "execution_count": 11, "outputs": [{"name": "stdout", "text": "gs://dll-data-bucket-697607461278/delta-consumable/:\ngs://dll-data-bucket-697607461278/delta-consumable/part-00000-35ab01b4-063e-4bd9-9490-0b80f3aea8dd-c000.snappy.parquet\n\ngs://dll-data-bucket-697607461278/delta-consumable/_delta_log/:\ngs://dll-data-bucket-697607461278/delta-consumable/_delta_log/\ngs://dll-data-bucket-697607461278/delta-consumable/_delta_log/00000000000000000000.json\n", "output_type": "stream"}], "id": "27de098c-817c-486a-9ad8-53be00d8e9bb"}, {"cell_type": "code", "source": "!gsutil cat $DELTA_LAKE_DIR_ROOT/_delta_log/00000000000000000000.json", "metadata": {"trusted": true}, "execution_count": 12, "outputs": [{"name": "stdout", "text": "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n{\"metaData\":{\"id\":\"fce6afad-4f03-49cb-aa6f-0605d968786b\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"addr_state\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"count\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1698870109769}}\n{\"add\":{\"path\":\"part-00000-35ab01b4-063e-4bd9-9490-0b80f3aea8dd-c000.snappy.parquet\",\"partitionValues\":{},\"size\":978,\"modificationTime\":1698870116822,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":51,\\\"minValues\\\":{\\\"addr_state\\\":\\\"AK\\\",\\\"count\\\":1},\\\"maxValues\\\":{\\\"addr_state\\\":\\\"WY\\\",\\\"count\\\":1},\\\"nullCount\\\":{\\\"addr_state\\\":0,\\\"count\\\":0}}\"}}\n{\"commitInfo\":{\"timestamp\":1698870120444,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"1\",\"numOutputRows\":\"51\",\"numOutputBytes\":\"978\"},\"engineInfo\":\"Apache-Spark/3.3.2 Delta-Lake/2.1.0\",\"txnId\":\"599b06c6-06df-431a-a3bd-07f672f7d074\"}}\n", "output_type": "stream"}], "id": "89cbc356-ebb9-4908-8193-c859fed0b2d0"}, {"cell_type": "markdown", "source": "### 5. Table Details\nhttps://docs.delta.io/latest/delta-utility.html#id6", "metadata": {}, "id": "276caa8b-dd60-48ac-b95c-c1302ec883af"}, {"cell_type": "code", "source": "deltaTable = DeltaTable.forPath(spark, DELTA_LAKE_DIR_ROOT)\ndetailDF = deltaTable.detail()\ndetailPDF=detailDF.toPandas()\ndetailPDF", "metadata": {"trusted": true}, "execution_count": 13, "outputs": [{"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}, {"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "  format                                    id  name description  \\\n0  delta  fce6afad-4f03-49cb-aa6f-0605d968786b  None        None   \n\n                                            location               createdAt  \\\n0  gs://dll-data-bucket-697607461278/delta-consum... 2023-11-01 20:21:49.769   \n\n             lastModified partitionColumns  numFiles  sizeInBytes properties  \\\n0 2023-11-01 20:22:00.822               []         1          978         {}   \n\n   minReaderVersion  minWriterVersion  \n0                 1                 2  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>format</th>\n      <th>id</th>\n      <th>name</th>\n      <th>description</th>\n      <th>location</th>\n      <th>createdAt</th>\n      <th>lastModified</th>\n      <th>partitionColumns</th>\n      <th>numFiles</th>\n      <th>sizeInBytes</th>\n      <th>properties</th>\n      <th>minReaderVersion</th>\n      <th>minWriterVersion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>delta</td>\n      <td>fce6afad-4f03-49cb-aa6f-0605d968786b</td>\n      <td>None</td>\n      <td>None</td>\n      <td>gs://dll-data-bucket-697607461278/delta-consum...</td>\n      <td>2023-11-01 20:21:49.769</td>\n      <td>2023-11-01 20:22:00.822</td>\n      <td>[]</td>\n      <td>1</td>\n      <td>978</td>\n      <td>{}</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "id": "fbe4fa8f-7fe8-44bc-a09f-a894852a901d"}, {"cell_type": "markdown", "source": "### 6. Table History\n\nhttps://docs.delta.io/latest/delta-utility.html#id4", "metadata": {}, "id": "c80b94f1-5750-4aed-a0dd-1fa20804ac1a"}, {"cell_type": "code", "source": "deltaTable = DeltaTable.forPath(spark, DELTA_LAKE_DIR_ROOT)\nfullHistoryPDF = deltaTable.history().toPandas()    # get the full history of the table\nlastOperationPDF = deltaTable.history(1).toPandas() # get the last operation\n\n", "metadata": {"trusted": true}, "execution_count": 14, "outputs": [{"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}], "id": "b767fc6d-5fb5-4364-ad62-84b9a69dd9f7"}, {"cell_type": "markdown", "source": "#### Last operation", "metadata": {}, "id": "5cdc034e-0bf6-4a5f-861d-20a96245ab5e"}, {"cell_type": "code", "source": "lastOperationPDF", "metadata": {"trusted": true}, "execution_count": 15, "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "   version               timestamp userId userName operation  \\\n0        0 2023-11-01 20:22:00.822   None     None     WRITE   \n\n                          operationParameters   job notebook clusterId  \\\n0  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n\n   readVersion isolationLevel  isBlindAppend  \\\n0          NaN   Serializable          False   \n\n                                    operationMetrics userMetadata  \\\n0  {'numOutputRows': '51', 'numOutputBytes': '978...         None   \n\n                            engineInfo  \n0  Apache-Spark/3.3.2 Delta-Lake/2.1.0  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>version</th>\n      <th>timestamp</th>\n      <th>userId</th>\n      <th>userName</th>\n      <th>operation</th>\n      <th>operationParameters</th>\n      <th>job</th>\n      <th>notebook</th>\n      <th>clusterId</th>\n      <th>readVersion</th>\n      <th>isolationLevel</th>\n      <th>isBlindAppend</th>\n      <th>operationMetrics</th>\n      <th>userMetadata</th>\n      <th>engineInfo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2023-11-01 20:22:00.822</td>\n      <td>None</td>\n      <td>None</td>\n      <td>WRITE</td>\n      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>Serializable</td>\n      <td>False</td>\n      <td>{'numOutputRows': '51', 'numOutputBytes': '978...</td>\n      <td>None</td>\n      <td>Apache-Spark/3.3.2 Delta-Lake/2.1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "id": "74cf9014-2e8d-4988-96a3-230ba4581ad0"}, {"cell_type": "markdown", "source": "#### Full History", "metadata": {}, "id": "c853d124-684f-47c7-920e-d73db1766cce"}, {"cell_type": "code", "source": "fullHistoryPDF", "metadata": {"trusted": true}, "execution_count": 16, "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "   version               timestamp userId userName operation  \\\n0        0 2023-11-01 20:22:00.822   None     None     WRITE   \n\n                          operationParameters   job notebook clusterId  \\\n0  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n\n   readVersion isolationLevel  isBlindAppend  \\\n0          NaN   Serializable          False   \n\n                                    operationMetrics userMetadata  \\\n0  {'numOutputRows': '51', 'numOutputBytes': '978...         None   \n\n                            engineInfo  \n0  Apache-Spark/3.3.2 Delta-Lake/2.1.0  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>version</th>\n      <th>timestamp</th>\n      <th>userId</th>\n      <th>userName</th>\n      <th>operation</th>\n      <th>operationParameters</th>\n      <th>job</th>\n      <th>notebook</th>\n      <th>clusterId</th>\n      <th>readVersion</th>\n      <th>isolationLevel</th>\n      <th>isBlindAppend</th>\n      <th>operationMetrics</th>\n      <th>userMetadata</th>\n      <th>engineInfo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2023-11-01 20:22:00.822</td>\n      <td>None</td>\n      <td>None</td>\n      <td>WRITE</td>\n      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>Serializable</td>\n      <td>False</td>\n      <td>{'numOutputRows': '51', 'numOutputBytes': '978...</td>\n      <td>None</td>\n      <td>Apache-Spark/3.3.2 Delta-Lake/2.1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "id": "6e49cb41-598f-46c5-b21e-9b79343e6b65"}, {"cell_type": "markdown", "source": "### 7. Table manifest file\nhttps://docs.delta.io/latest/delta-utility.html#id8\n\nYou can a generate manifest file for a Delta table that can be used by other processing engines (that is, other than Apache Spark) to read the Delta table. For example, to generate a manifest file that can be used by Presto and Athena to read a Delta table, you run the following:", "metadata": {}, "id": "073428e0-5141-4c34-82e3-786224d7b878"}, {"cell_type": "code", "source": "deltaTable = DeltaTable.forPath(spark, DELTA_LAKE_DIR_ROOT)\ndeltaTable.generate(\"symlink_format_manifest\")", "metadata": {"trusted": true}, "execution_count": 17, "outputs": [{"name": "stderr", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/spark/conf/ivysettings.xml will be used\n                                                                                \r", "output_type": "stream"}], "id": "685f24f4-6569-453a-b39b-7ab1aeefad40"}, {"cell_type": "code", "source": "!gsutil ls -r $DELTA_LAKE_DIR_ROOT | grep \"_symlink_format_manifest/manifest\"", "metadata": {"trusted": true}, "execution_count": 18, "outputs": [{"name": "stdout", "text": "gs://dll-data-bucket-697607461278/delta-consumable/_symlink_format_manifest/manifest\n", "output_type": "stream"}], "id": "e0c7b2c6-7ae1-42ca-bf57-cfa7603757cd"}, {"cell_type": "code", "source": "MANIFEST_LIST = !gsutil ls -r $DELTA_LAKE_DIR_ROOT | grep \"_symlink_format_manifest/manifest\"\nMANIFEST_FILE = MANIFEST_LIST[0]\nprint(MANIFEST_FILE)", "metadata": {"trusted": true}, "execution_count": 19, "outputs": [{"name": "stdout", "text": "gs://dll-data-bucket-697607461278/delta-consumable/_symlink_format_manifest/manifest\n", "output_type": "stream"}], "id": "d4dfb0d4-342f-4651-a24c-7097fb2b13f6"}, {"cell_type": "code", "source": "!gsutil cat $MANIFEST_FILE", "metadata": {"trusted": true}, "execution_count": 20, "outputs": [{"name": "stdout", "text": "gs://dll-data-bucket-697607461278/delta-consumable/part-00000-35ab01b4-063e-4bd9-9490-0b80f3aea8dd-c000.snappy.parquet\n", "output_type": "stream"}], "id": "678cc704-db99-4941-bfbd-af363a5f4978"}, {"cell_type": "markdown", "source": "Using this manifest file, you can create an external table in BigQuery on the Delta Table, except it will be point in time to when the manifest was generated.", "metadata": {}, "id": "f2a85df4-20f0-4829-be51-b45639b2ad5a"}, {"cell_type": "markdown", "source": "#### First let's look at the delta table via Spark", "metadata": {"tags": []}, "id": "78e7c633-f34c-4da1-b7cb-250f94b5d3cc"}, {"cell_type": "code", "source": "spark.sql(\"show tables from loan_db;\").show(truncate=False)\ndf = deltaTable.toDF()\npdf = df.toPandas()\npdf = pdf.sort_values(by='addr_state', ascending=True)\nprint(pdf)\n", "metadata": {"trusted": true}, "execution_count": 21, "outputs": [{"name": "stderr", "text": "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\nANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\nANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\nANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n", "output_type": "stream"}, {"name": "stdout", "text": "+---------+----------------------+-----------+\n|namespace|tableName             |isTemporary|\n+---------+----------------------+-----------+\n|loan_db  |loans_by_state_delta  |false      |\n|loan_db  |loans_by_state_parquet|false      |\n|loan_db  |loans_cleansed_parquet|false      |\n+---------+----------------------+-----------+\n\n", "output_type": "stream"}, {"name": "stderr", "text": "[Stage 25:>                                                         (0 + 1) / 1]\r", "output_type": "stream"}, {"name": "stdout", "text": "   addr_state  count\n46         AK      1\n30         AL      1\n47         AR      1\n0          AZ      1\n16         CA      1\n45         CO      1\n17         CT      1\n5          DC      1\n23         DE      1\n44         FL      1\n41         GA      1\n50         HI      1\n35         IA      1\n15         ID      1\n25         IL      1\n31         IN      1\n43         KS      1\n9          KY      1\n2          LA      1\n42         MA      1\n22         MD      1\n26         ME      1\n12         MI      1\n3          MN      1\n24         MO      1\n29         MS      1\n19         MT      1\n20         NC      1\n28         ND      1\n18         NE      1\n11         NH      1\n4          NJ      1\n34         NM      1\n13         NV      1\n38         NY      1\n32         OH      1\n48         OK      1\n6          OR      1\n36         PA      1\n8          RI      1\n1          SC      1\n37         SD      1\n33         TN      1\n39         TX      1\n49         UT      1\n7          VA      1\n21         VT      1\n27         WA      1\n14         WI      1\n40         WV      1\n10         WY      1\n", "output_type": "stream"}, {"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}], "id": "b9d7f123-8950-485b-bb04-9020e29def8b"}, {"cell_type": "markdown", "source": "#### Now let's create a BigLake table using the manifest file created above", "metadata": {"tags": []}, "id": "b8c09661-a020-4bbb-bc3b-baef836bb867"}, {"cell_type": "code", "source": "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\ndef bq_query(sql, show_job_id=False):\n    \"\"\"\n    Input: SQL query, as a string, to execute in BigQuery\n    Returns the query results as a pandas DataFrame, or error, if any\n    \"\"\"\n\n    # Try dry run before executing query to catch any errors\n    try:\n        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n\n        bq_client.query(sql, job_config=job_config)\n\n    except BadRequest as err:\n        print(err)\n        return\n\n    job_config = bigquery.QueryJobConfig()\n    client_result = bq_client.query(sql, job_config=job_config)\n\n    job_id = client_result.job_id\n\n    # Wait for query/job to finish running, then get & return data frame\n    df = client_result.result().to_dataframe()\n\n    if show_job_id:\n        print(f\"Finished job_id: {job_id}\")\n\n    return df", "metadata": {"trusted": true}, "execution_count": 22, "outputs": [], "id": "5f3e6230-3ff3-4db0-9b6d-cd23a93da400"}, {"cell_type": "markdown", "source": "#### First, let's build and print out the BigQuery command to create an external table with the delta manifest file", "metadata": {"tags": []}, "id": "cfa43dfc-4b18-413d-a677-c4daf6d5b3c0"}, {"cell_type": "code", "source": "\nbq_client = bigquery.Client(project=PROJECT_ID)\n\nDATASET = 'delta_dataset'\nCONNECTION = 'us.biglake-connection'\nTABLE_NAME = 'bq_delta_table'\nURI = 'gs://dll-data-bucket-' + PROJECT_NUMBER + '/delta-consumable/_symlink_format_manifest/manifest'\nBQSQL = 'CREATE EXTERNAL TABLE ' + DATASET + '.' + TABLE_NAME + ' ' + \\\n        'WITH CONNECTION `' +  CONNECTION + '` ' + \\\n        'OPTIONS (' + \\\n        'format=\"PARQUET\", ' + \\\n        'uris=[\"' + URI + '\"],' + \\\n        'file_set_spec_type = \\'NEW_LINE_DELIMITED_MANIFEST\\',' + \\\n        'max_staleness = INTERVAL 1 DAY,' + \\\n        'metadata_cache_mode = \\'AUTOMATIC\\'' + \\\n        ');'\n\n\nprint(sqlparse.format(BQSQL, reindent_aligned = True, keyword_case='upper'))\n\n\n", "metadata": {"tags": [], "trusted": true}, "execution_count": 23, "outputs": [{"name": "stdout", "text": "CREATE EXTERNAL TABLE delta_dataset.bq_delta_table WITH\nCONNECTION `us.biglake-connection`\nOPTIONS (format=\"PARQUET\", uris=[\"gs://dll-data-bucket-697607461278/delta-consumable/_symlink_format_manifest/manifest\"],file_set_spec_type = 'NEW_LINE_DELIMITED_MANIFEST',max_staleness = INTERVAL 1 DAY,metadata_cache_mode = 'AUTOMATIC');\n", "output_type": "stream"}], "id": "aa6fc2eb-8017-4fc6-9688-4a7414bf5369"}, {"cell_type": "code", "source": "#### Now, you can execute the BigQuery SQL above in the BigQuery Console as shown below or you can execute the next cells to create the table for you", "metadata": {"trusted": true}, "execution_count": 24, "outputs": [], "id": "cdb54dc0-1cf9-47e7-b3e0-2b20093379b2"}, {"cell_type": "code", "source": "\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nImage(url = \"./images/Lab3-Image1.png\")\n", "metadata": {"trusted": true}, "execution_count": 25, "outputs": [{"execution_count": 25, "output_type": "execute_result", "data": {"text/html": "<img src=\"./images/Lab3-Image1.png\"/>", "text/plain": "<IPython.core.display.Image object>"}, "metadata": {}}], "id": "5d6d623c-ba20-4dd8-90db-7876e4d90498"}, {"cell_type": "code", "source": "df = bq_query(BQSQL)\nprint(df)", "metadata": {"trusted": true}, "execution_count": 26, "outputs": [{"name": "stdout", "text": "Empty DataFrame\nColumns: []\nIndex: []\n", "output_type": "stream"}], "id": "3bd4b5e7-0138-4cc0-9d20-e295e8215f80"}, {"cell_type": "markdown", "source": "#### Now Query the delta table that you created above", "metadata": {"tags": []}, "id": "b163a11e-8070-44e1-8c92-0fb6fa7b056f"}, {"cell_type": "code", "source": "BQDELTASQL = 'select addr_state,count from '  + DATASET + '.' + TABLE_NAME + ' order by addr_state ASC;'\ndf = bq_query(BQDELTASQL)\nprint(df)", "metadata": {"trusted": true}, "execution_count": 27, "outputs": [{"name": "stdout", "text": "   addr_state  count\n0          AK      1\n1          AL      1\n2          AR      1\n3          AZ      1\n4          CA      1\n5          CO      1\n6          CT      1\n7          DC      1\n8          DE      1\n9          FL      1\n10         GA      1\n11         HI      1\n12         IA      1\n13         ID      1\n14         IL      1\n15         IN      1\n16         KS      1\n17         KY      1\n18         LA      1\n19         MA      1\n20         MD      1\n21         ME      1\n22         MI      1\n23         MN      1\n24         MO      1\n25         MS      1\n26         MT      1\n27         NC      1\n28         ND      1\n29         NE      1\n30         NH      1\n31         NJ      1\n32         NM      1\n33         NV      1\n34         NY      1\n35         OH      1\n36         OK      1\n37         OR      1\n38         PA      1\n39         RI      1\n40         SC      1\n41         SD      1\n42         TN      1\n43         TX      1\n44         UT      1\n45         VA      1\n46         VT      1\n47         WA      1\n48         WI      1\n49         WV      1\n50         WY      1\n", "output_type": "stream"}], "id": "0d3ca03d-8e31-4e14-b4e9-0fdceca8aa16"}, {"cell_type": "markdown", "source": "#### Create manifest file for partitioned delta table", "metadata": {"tags": []}, "id": "47c4b87e-7aa2-4eb3-bfb9-f2c4aaba7224"}, {"cell_type": "code", "source": "DELTA_PARTLAKE_DIR_ROOT = f\"gs://dll-data-bucket-{PROJECT_NUMBER}/delta-sample-partitioned\"\ndeltaPartTable = DeltaTable.forPath(spark, DELTA_PARTLAKE_DIR_ROOT)\ndeltaPartTable.generate(f\"symlink_format_manifest\")", "metadata": {"trusted": true}, "execution_count": 28, "outputs": [{"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}], "id": "9cc23458-e315-4450-bcc9-9f8f969dceee"}, {"cell_type": "markdown", "source": "#### First let's look at the delta partitioned table via Spark", "metadata": {"tags": []}, "id": "894eef90-9014-4a4f-912b-7589ab82c401"}, {"cell_type": "code", "source": "df = deltaPartTable.toDF()\npdf = df.toPandas()\npdf = pdf.sort_values(by='addr_state', ascending=True)\nprint(pdf)", "metadata": {"trusted": true}, "execution_count": 29, "outputs": [{"name": "stderr", "text": "[Stage 44:=============================>                            (4 + 4) / 8]\r", "output_type": "stream"}, {"name": "stdout", "text": "   addr_state  count\n6          AK      1\n17         AL      1\n3          AR      1\n9          AZ      1\n4          CA      1\n40         CO      1\n48         CT      1\n22         DC      1\n27         DE      1\n50         FL      1\n45         GA      1\n10         HI      1\n30         IA      1\n32         ID      1\n28         IL      1\n39         IN      1\n25         KS      1\n5          KY      1\n41         LA      1\n34         MA      1\n0          MD      1\n16         ME      1\n31         MI      1\n36         MN      1\n21         MO      1\n11         MS      1\n33         MT      1\n20         NC      1\n1          ND      1\n46         NE      1\n8          NH      1\n26         NJ      1\n35         NM      1\n38         NV      1\n23         NY      1\n15         OH      1\n12         OK      1\n7          OR      1\n44         PA      1\n37         RI      1\n24         SC      1\n13         SD      1\n14         TN      1\n42         TX      1\n47         UT      1\n49         VA      1\n43         VT      1\n2          WA      1\n19         WI      1\n29         WV      1\n18         WY      1\n", "output_type": "stream"}, {"name": "stderr", "text": "                                                                                \r", "output_type": "stream"}], "id": "417ec27a-737a-4938-b463-29653f261174"}, {"cell_type": "markdown", "source": "#### Now let's create a BigLake partitioned table using the manifest file created above", "metadata": {"tags": []}, "id": "17bc08e3-5df3-4bbb-a00d-b97802a0e388"}, {"cell_type": "code", "source": "#### First, let's build and print out the BigQuery command to create an external partitioned table with the delta manifest file", "metadata": {"trusted": true}, "execution_count": 30, "outputs": [], "id": "7786484e-1218-40b8-9bd9-e691e120b021"}, {"cell_type": "code", "source": "DATASET = 'delta_dataset'\nCONNECTION = 'us.biglake-connection'\nTABLE_NAME = 'bq_delta_partitioned_table'\nURI = 'gs://dll-data-bucket-' + PROJECT_NUMBER + '/delta-sample-partitioned/_symlink_format_manifest/*/manifest'\nBQSQL = 'CREATE EXTERNAL TABLE ' + DATASET + '.' + TABLE_NAME + ' ' + \\\n        'WITH PARTITION COLUMNS(addr_state STRING)' + ' ' + \\\n        'WITH CONNECTION `' +  CONNECTION + '` ' + \\\n        'OPTIONS (' + \\\n        'format=\"PARQUET\", ' + \\\n        'hive_partition_uri_prefix=\"' + DELTA_PARTLAKE_DIR_ROOT + '\", ' + \\\n        'uris=[\"' + URI + '\"],' + \\\n        'file_set_spec_type = \\'NEW_LINE_DELIMITED_MANIFEST\\',' + \\\n        'max_staleness = INTERVAL 1 DAY,' + \\\n        'metadata_cache_mode = \\'AUTOMATIC\\'' + \\\n        ');'\n\n\n\nprint(sqlparse.format(BQSQL, reindent_aligned = True, keyword_case='upper'))\n\n", "metadata": {"trusted": true}, "execution_count": 31, "outputs": [{"name": "stdout", "text": "CREATE EXTERNAL TABLE delta_dataset.bq_delta_partitioned_table WITH\nPARTITION COLUMNS(addr_state STRING) WITH\nCONNECTION `us.biglake-connection`\nOPTIONS (format=\"PARQUET\", hive_partition_uri_prefix=\"gs://dll-data-bucket-697607461278/delta-sample-partitioned\", uris=[\"gs://dll-data-bucket-697607461278/delta-sample-partitioned/_symlink_format_manifest/*/manifest\"],file_set_spec_type = 'NEW_LINE_DELIMITED_MANIFEST',max_staleness = INTERVAL 1 DAY,metadata_cache_mode = 'AUTOMATIC');\n", "output_type": "stream"}], "id": "93fb8dc1-4a76-4a6c-acf5-a60cebb9436d"}, {"cell_type": "code", "source": "#### Now, you can execute the BigQuery SQL above in the BigQuery Console as shown below or you can execute the next cells to create the partitioned table for you", "metadata": {"trusted": true}, "execution_count": 32, "outputs": [], "id": "85bd3a5f-236f-46e3-aff0-1de7d893376d"}, {"cell_type": "code", "source": "\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nImage(url = \"./images/Lab3-Image2.png\")", "metadata": {"trusted": true}, "execution_count": 33, "outputs": [{"execution_count": 33, "output_type": "execute_result", "data": {"text/html": "<img src=\"./images/Lab3-Image2.png\"/>", "text/plain": "<IPython.core.display.Image object>"}, "metadata": {}}], "id": "9fa05cc8-773c-4da1-b8fc-d89e879339b4"}, {"cell_type": "code", "source": "df = bq_query(BQSQL)\nprint(df)\n", "metadata": {}, "execution_count": null, "outputs": [], "id": "d16b50e9-8ed6-4a99-a2d2-7dbe87e8e4c4"}, {"cell_type": "markdown", "source": "#### Now Query the delta table that you created above", "metadata": {"tags": []}, "id": "a0705c82-df38-4719-ba1f-9c9b41f0f3fa"}, {"cell_type": "code", "source": "BQDELTASQL = 'select addr_state,count from '  + DATASET + '.' + TABLE_NAME + ' order by addr_state ASC;'\ndf = bq_query(BQDELTASQL)\nprint(df)", "metadata": {}, "execution_count": null, "outputs": [], "id": "75278b60-d8b6-4b39-b7ed-a0fe70d2d919"}, {"cell_type": "markdown", "source": "### 8. Hive Metastore Entry", "metadata": {}, "id": "7da0d8c0-a73e-400a-9e32-306a9f7fc4e6"}, {"cell_type": "code", "source": "spark.sql(\"show tables in loan_db\").show(truncate=False)", "metadata": {}, "execution_count": 17, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+----------------------+-----------+\n\n\n\n|namespace|tableName             |isTemporary|\n\n\n\n+---------+----------------------+-----------+\n\n\n\n|loan_db  |loans_by_state_delta  |false      |\n\n\n\n|loan_db  |loans_by_state_parquet|false      |\n\n\n\n|loan_db  |loans_cleansed_parquet|false      |\n\n\n\n+---------+----------------------+-----------+\n\n\n\n\n"}], "id": "7cca2f6d-029c-4cf3-ae9d-9c7983e36fd7"}, {"cell_type": "markdown", "source": "### THIS CONCLUDES THIS UNIT. PROCEED TO THE NEXT NOTEBOOK", "metadata": {}, "id": "e3e82375-f053-458b-bd5a-c4414b832f29"}]}
