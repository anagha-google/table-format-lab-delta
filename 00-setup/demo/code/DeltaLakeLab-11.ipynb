{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4529a790-8425-4600-841a-4b094b82eaa8",
   "metadata": {},
   "source": [
    "# Delta Lake Lab \n",
    "## Unit 11: Using UniForm to seamlessly replicate Delta Lake commits as Iceberg commits & reading Delta Lake as Iceberg tables\n",
    "\n",
    "This lab is powered by Dataproc Serverless Spark.\n",
    "\n",
    "\n",
    "In this unit, we will -\n",
    "1. Create a UniForm enabled Delta Lake table with Iceberg as the target metadata format\n",
    "2. We will query the Delta Lake table\n",
    "3. We will query the Iceberg table\n",
    "4. We will update the Delta Lake table and then query the Iceberg table to see if the changes are available when queries from the corresspoding Iceberg table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31c4fd-d465-4f52-8e56-3775bf499abc",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1321bce9-178c-4065-8187-0a5728c1a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import month, date_format\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "from google.cloud.exceptions import BadRequest\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import sqlparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295a74-ed1d-4b5d-831a-1b5dcf73c36f",
   "metadata": {},
   "source": [
    "### 2. Create a Spark session powered by Cloud Dataproc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b383d5ab-a0b9-45ab-a232-34d88f2a0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/03 02:03:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gdpic-srvls-session-7496b9aa-c1b2-4af0-b19a-eed494ea3479-m.us-central1-c.c.delta-lake-diy-lab.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://gdpic-srvls-session-7496b9aa-c1b2-4af0-b19a-eed494ea3479-m:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4592f00b90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Loan Analysis').config('spark.databricks.delta.write.dataFilesToSubdir',True).config('spark.databricks.delta.allowArbitraryProperties.enabled', True).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd13e6-f3f5-4f2c-b4fc-d7e2660c6206",
   "metadata": {},
   "source": [
    "### 3. Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5596e31b-749a-4702-8879-6f05f9ff0c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID:  delta-lake-diy-lab\n"
     ]
    }
   ],
   "source": [
    "project_id_output = !gcloud config list --format \"value(core.project)\" 2>/dev/null\n",
    "PROJECT_ID = project_id_output[0]\n",
    "print(\"PROJECT_ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471c6743-a058-462b-851a-f34323f36243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_NAME:  delta-lake-diy-lab\n"
     ]
    }
   ],
   "source": [
    "project_name_output = !gcloud projects describe $PROJECT_ID | grep name | cut -d':' -f2 | xargs\n",
    "PROJECT_NAME = project_name_output[0]\n",
    "print(\"PROJECT_NAME: \", PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61929dc6-a083-433c-8a13-3d39d9c4a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_NUMBER:  11002190840\n"
     ]
    }
   ],
   "source": [
    "project_number_output = !gcloud projects describe $PROJECT_ID | grep projectNumber | cut -d':' -f2 | xargs\n",
    "PROJECT_NUMBER = project_number_output[0]\n",
    "print(\"PROJECT_NUMBER: \", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b79fd2-5243-41a4-ae87-e7f9dd87cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LAKE_ROOT_PATH= f\"gs://dll-data-bucket-{PROJECT_NUMBER}\"\n",
    "DELTA_LAKE_AND_ICEBERG_DIR = f\"{DATA_LAKE_ROOT_PATH}/delta-uniform-iceberg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073428e0-5141-4c34-82e3-786224d7b878",
   "metadata": {},
   "source": [
    "### 4. Create a UniForm enabled Delta Lake table with target format of Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685f24f4-6569-453a-b39b-7ab1aeefad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/spark/conf/ivysettings.xml will be used\n",
      "23/12/03 02:04:06 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create delta dataset from the Parquet table\n",
    "spark.sql(\"SELECT addr_state,count(*) as count FROM loan_db.loans_by_state_parquet group by addr_state\").write.mode(\"overwrite\").format(\"delta\").save(f\"{DELTA_LAKE_AND_ICEBERG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8b4f2b-92a2-4681-a3d6-cd961fa7857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/03 02:05:24 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`loan_db`.`loans_by_state_delta_uniform` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "23/12/03 02:05:24 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register a UniForm enabled table into the HMS, on the Delta Lake dataset above, with Iceberg as the target format for metadata translation\n",
    "# Define external delta table definition\n",
    "spark.sql(\"DROP TABLE IF EXISTS loan_db.loans_by_state_delta_uniform;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE loan_db.loans_by_state_delta_uniform USING delta LOCATION \\\"{DELTA_LAKE_AND_ICEBERG_DIR}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3915e305-a423-4600-a393-5210fef77ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are setting a property: delta.enableIcebergCompatV1 that is not recognized by this version of Delta\n",
      "You are setting a property: delta.universalFormat.enabledFormats that is not recognized by this version of Delta\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o82.sql.\n: org.apache.spark.sql.delta.DeltaColumnMappingUnsupportedException: \nYour current table protocol version does not support changing column mapping modes\nusing delta.columnMapping.mode.\n\nRequired Delta protocol version for column mapping:\nProtocol(2,5)\nYour table's current Delta protocol version:\nProtocol(1,2)\n\nPlease enable Column Mapping on your Delta table with mapping mode 'name'.\nYou can use one of the following commands.\n\nIf your table is already on the required protocol version:\nALTER TABLE table_name SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\n\nIf your table is not on the required protocol version and requires a protocol upgrade:\nALTER TABLE table_name SET TBLPROPERTIES (\n   'delta.columnMapping.mode' = 'name',\n   'delta.minReaderVersion' = '2',\n   'delta.minWriterVersion' = '5')\n\n\tat org.apache.spark.sql.delta.DeltaErrorsBase.changeColumnMappingModeOnOldProtocol(DeltaErrors.scala:1932)\n\tat org.apache.spark.sql.delta.DeltaErrorsBase.changeColumnMappingModeOnOldProtocol$(DeltaErrors.scala:1914)\n\tat org.apache.spark.sql.delta.DeltaErrors$.changeColumnMappingModeOnOldProtocol(DeltaErrors.scala:2794)\n\tat org.apache.spark.sql.delta.DeltaColumnMappingBase.verifyAndUpdateMetadataChange(DeltaColumnMapping.scala:127)\n\tat org.apache.spark.sql.delta.DeltaColumnMappingBase.verifyAndUpdateMetadataChange$(DeltaColumnMapping.scala:93)\n\tat org.apache.spark.sql.delta.DeltaColumnMapping$.verifyAndUpdateMetadataChange(DeltaColumnMapping.scala:611)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:415)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:395)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:137)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:379)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:372)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:137)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.$anonfun$run$1(alterDeltaTableCommands.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.recordFrameProfile(alterDeltaTableCommands.scala:99)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.recordOperation(alterDeltaTableCommands.scala:99)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.recordDeltaOperation(alterDeltaTableCommands.scala:99)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.run(alterDeltaTableCommands.scala:106)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$alterTable$3(DeltaCatalog.scala:571)\n\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1076)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$alterTable$1(DeltaCatalog.scala:539)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.alterTable(DeltaCatalog.scala:521)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.alterTable(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.execution.datasources.v2.AlterTableExec.run(AlterTableExec.scala:37)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE loan_db.loans_by_state_delta_uniform SET TBLPROPERTIES(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta.columnMapping.mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta.enableIcebergCompatV1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta.universalFormat.enabledFormats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m);\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o82.sql.\n: org.apache.spark.sql.delta.DeltaColumnMappingUnsupportedException: \nYour current table protocol version does not support changing column mapping modes\nusing delta.columnMapping.mode.\n\nRequired Delta protocol version for column mapping:\nProtocol(2,5)\nYour table's current Delta protocol version:\nProtocol(1,2)\n\nPlease enable Column Mapping on your Delta table with mapping mode 'name'.\nYou can use one of the following commands.\n\nIf your table is already on the required protocol version:\nALTER TABLE table_name SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\n\nIf your table is not on the required protocol version and requires a protocol upgrade:\nALTER TABLE table_name SET TBLPROPERTIES (\n   'delta.columnMapping.mode' = 'name',\n   'delta.minReaderVersion' = '2',\n   'delta.minWriterVersion' = '5')\n\n\tat org.apache.spark.sql.delta.DeltaErrorsBase.changeColumnMappingModeOnOldProtocol(DeltaErrors.scala:1932)\n\tat org.apache.spark.sql.delta.DeltaErrorsBase.changeColumnMappingModeOnOldProtocol$(DeltaErrors.scala:1914)\n\tat org.apache.spark.sql.delta.DeltaErrors$.changeColumnMappingModeOnOldProtocol(DeltaErrors.scala:2794)\n\tat org.apache.spark.sql.delta.DeltaColumnMappingBase.verifyAndUpdateMetadataChange(DeltaColumnMapping.scala:127)\n\tat org.apache.spark.sql.delta.DeltaColumnMappingBase.verifyAndUpdateMetadataChange$(DeltaColumnMapping.scala:93)\n\tat org.apache.spark.sql.delta.DeltaColumnMapping$.verifyAndUpdateMetadataChange(DeltaColumnMapping.scala:611)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:415)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:395)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:137)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:379)\n\tat org.apache.spark.sql.delta.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:372)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:137)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.$anonfun$run$1(alterDeltaTableCommands.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.recordFrameProfile(alterDeltaTableCommands.scala:99)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.recordOperation(alterDeltaTableCommands.scala:99)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.recordDeltaOperation(alterDeltaTableCommands.scala:99)\n\tat org.apache.spark.sql.delta.commands.AlterTableSetPropertiesDeltaCommand.run(alterDeltaTableCommands.scala:106)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$alterTable$3(DeltaCatalog.scala:571)\n\tat scala.collection.immutable.HashMap.foreach(HashMap.scala:1076)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$alterTable$1(DeltaCatalog.scala:539)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.alterTable(DeltaCatalog.scala:521)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.alterTable(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.execution.datasources.v2.AlterTableExec.run(AlterTableExec.scala:37)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE loan_db.loans_by_state_delta_uniform SET TBLPROPERTIES('delta.columnMapping.mode' = 'name','delta.enableIcebergCompatV1' = 'true','delta.universalFormat.enabledFormats' = 'iceberg');\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983ac17-0e17-48d6-b6aa-6b780b6e0dde",
   "metadata": {},
   "source": [
    "#### 4.2. Create a manifest file on the partitioned loans dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c09661-a020-4bbb-bc3b-baef836bb867",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6.2. Create BigLake table DDL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516d10a-eb48-4f4e-8f63-e8ef0fdbe527",
   "metadata": {},
   "source": [
    "#### 6.3. Create the BigLake unpartitioned manifest based Delta Lake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163a11e-8070-44e1-8c92-0fb6fa7b056f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6.4. Run a query against the BigLake unpartitioned Delta Lake table using the BigQuery Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3d40-b52c-41d4-86fe-f40db0b69522",
   "metadata": {},
   "source": [
    "### 7. Create and query a BigLake table on the partitioned Delta Lake table manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b21e2-b7d1-455a-b75f-c320b8065096",
   "metadata": {},
   "source": [
    "#### 7.1. Create an external table in Spark on the data and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fb756-1fbb-45b0-85f2-6f77da69a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define external delta table definition\n",
    "spark.sql(\"DROP TABLE IF EXISTS loan_db.loans_by_state_delta_partitioned;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE loan_db.loans_by_state_delta_partitioned USING delta LOCATION \\\"{PARTITIONED_DELTA_LAKE_DIR}\\\"\").show(truncate=False)\n",
    "spark.sql(\"SHOW TABLES IN loan_db\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b75231-0408-4041-8423-9d76806d1a03",
   "metadata": {},
   "source": [
    "#### 7.2. Explore the table data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0fce0-c23d-4a97-9d5b-f73e8d7a1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM loan_db.loans_by_state_delta_partitioned LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4b87e-7aa2-4eb3-bfb9-f2c4aaba7224",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 7.3. Create a BigLake table defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb8dc1-4a76-4a6c-acf5-a60cebb9436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'delta_dataset'\n",
    "CONNECTION = 'us.biglake-connection'\n",
    "TABLE_NAME = 'loans_deltalake_partitioned'\n",
    "URI = 'gs://dll-data-bucket-' + PROJECT_NUMBER + '/delta-sample-partitioned/_symlink_format_manifest/*/manifest'\n",
    "BIGLAKE_PARTITIONED_TABLE_DDL = 'CREATE OR REPLACE EXTERNAL TABLE ' + DATASET + '.' + TABLE_NAME + ' ' + \\\n",
    "        'WITH PARTITION COLUMNS(addr_state STRING)' + ' ' + \\\n",
    "        'WITH CONNECTION `' +  CONNECTION + '` ' + \\\n",
    "        'OPTIONS (' + \\\n",
    "        'format=\"PARQUET\", ' + \\\n",
    "        'hive_partition_uri_prefix=\"' + PARTITIONED_DELTA_LAKE_DIR + '\", ' + \\\n",
    "        'uris=[\"' + URI + '\"],' + \\\n",
    "        'file_set_spec_type = \\'NEW_LINE_DELIMITED_MANIFEST\\',' + \\\n",
    "        'max_staleness = INTERVAL 1 DAY,' + \\\n",
    "        'metadata_cache_mode = \\'AUTOMATIC\\'' + \\\n",
    "        ');'\n",
    "\n",
    "\n",
    "\n",
    "print(sqlparse.format(BIGLAKE_PARTITIONED_TABLE_DDL, reindent_aligned = True, keyword_case='upper'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00c367-f445-4b48-82ef-c155ae547049",
   "metadata": {},
   "source": [
    "#### 7.4. Create the BigLake partitioned manifest based Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b50e9-8ed6-4a99-a2d2-7dbe87e8e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can execute the SQL in the BQ UI; The following shows how to create the table using the BQ Python SDK; Returns a Pandas dataframe\n",
    "PDF = fn_execute_bq_statement(BIGLAKE_PARTITIONED_TABLE_DDL)\n",
    "print(PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0705c82-df38-4719-ba1f-9c9b41f0f3fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 7.5. Run a query against the BigLake partitioned Delta Lake table using the BigQuery Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75278b60-d8b6-4b39-b7ed-a0fe70d2d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGLAKE_SELECT_SQL = 'select addr_state,count from '  + DATASET + '.' + TABLE_NAME + ' LIMIT 5;'\n",
    "print(BIGLAKE_SELECT_SQL)\n",
    "PDF = fn_execute_bq_statement(BIGLAKE_SELECT_SQL)\n",
    "print(PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e82375-f053-458b-bd5a-c4414b832f29",
   "metadata": {},
   "source": [
    "### THIS CONCLUDES THIS UNIT. PROCEED TO THE NEXT NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "serverless_spark": "{\"name\":\"projects/delta-lake-diy-lab/locations/us-central1/sessions/delta-lake-lab-17460\",\"uuid\":\"7496b9aa-c1b2-4af0-b19a-eed494ea3479\",\"createTime\":\"2023-12-02T23:10:14.351363Z\",\"jupyterSession\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://in7kggl6mfcxxdldiezehugb4i-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/\",\"BYOID Spark History Server\":\"https://in7kggl6mfcxxdldiezehugb4i-dot-us-central1.dataproc.byoid.googleusercontent.com/sparkhistory/\"}},\"state\":\"ACTIVE\",\"stateTime\":\"2023-12-02T23:13:27.294546Z\",\"creator\":\"admin@akhanolkar.altostrat.com\",\"runtimeConfig\":{\"version\":\"2.1.27\",\"properties\":{\"spark:spark.jars.packages\":\"io.delta:delta-core_2.13:2.4.0\",\"spark:spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\"spark:spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.driver.memory\":\"9600m\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.executor.memory\":\"9600m\",\"spark:spark.dynamicAllocation.executorAllocationRatio\":\"0.3\",\"spark:spark.eventLog.dir\":\"gs://dll-sphs-bucket-11002190840/7496b9aa-c1b2-4af0-b19a-eed494ea3479/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"dll-lab-sa@delta-lake-diy-lab.iam.gserviceaccount.com\",\"subnetworkUri\":\"spark-snet\",\"idleTtl\":\"3600s\",\"ttl\":\"86400s\"},\"peripheralsConfig\":{\"metastoreService\":\"projects/delta-lake-diy-lab/locations/us-central1/services/dll-hms-11002190840\",\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/delta-lake-diy-lab/regions/us-central1/clusters/dll-sphs-11002190840\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2023-12-02T23:10:14.351363Z\"}]}",
  "serverless_spark_kernel_name": "remote-d973f900c0bcdc8a7fcee568-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
